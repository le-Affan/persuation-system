{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "20e27425",
   "metadata": {},
   "source": [
    "ADAPTIVE PERSUASION SYSTEM - PRODUCTION VERSION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7b07bb25",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All required libraries are installed and imported successfully.\n"
     ]
    }
   ],
   "source": [
    "!pip install -q transformers>=4.40.0 huggingface_hub>=0.23.0 torch\n",
    "!pip install -q textblob nltk pandas numpy matplotlib\n",
    "\n",
    "import torch, transformers, textblob, nltk, pandas, numpy, matplotlib\n",
    "print(\"All required libraries are installed and imported successfully.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "63d84b7e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Imports\n",
    "\n",
    "import os\n",
    "import json\n",
    "import re\n",
    "from datetime import datetime\n",
    "from typing import Dict, List, Tuple, Optional\n",
    "import random\n",
    "\n",
    "from huggingface_hub import InferenceClient, login\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "import torch\n",
    "from textblob import TextBlob\n",
    "import nltk\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import display, HTML\n",
    "\n",
    "\n",
    "# NLTK Downloads\n",
    "nltk.download('punkt', quiet=True)\n",
    "nltk.download('vader_lexicon', quiet=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "69af6cc9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Note: Environment variable`HF_TOKEN` is set and is the current active token independently from the token you've just configured.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Authenticated\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Authentication\n",
    "\n",
    "HF_TOKEN = os.getenv(\"HF_TOKEN\")\n",
    "os.environ['HF_TOKEN'] = HF_TOKEN\n",
    "\n",
    "try:\n",
    "    login(token=HF_TOKEN, add_to_git_credential=False)\n",
    "    print(\"Authenticated\\n\")\n",
    "except Exception as e:\n",
    "    print(f\"Failed: {e}\")\n",
    "    raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "95269a92",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Could not load model locally: You are trying to access a gated repo.\n",
      "Make sure to have access to it at https://huggingface.co/meta-llama/Llama-3.1-8B-Instruct.\n",
      "403 Client Error. (Request ID: Root=1-696264d4-64e04ebe6122323f382652f4;cbdba31c-ed6c-4434-822a-b0e65e5d713e)\n",
      "\n",
      "Cannot access gated repo for url https://huggingface.co/meta-llama/Llama-3.1-8B-Instruct/resolve/main/config.json.\n",
      "Your request to access model meta-llama/Llama-3.1-8B-Instruct is awaiting a review from the repo authors.\n",
      "Falling back to Inference API until access is approved\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Loading the llama 3.1 model\n",
    "\n",
    "MODEL_NAME = \"meta-llama/Llama-3.1-8B-Instruct\"\n",
    "\n",
    "try:\n",
    "    tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME, token=HF_TOKEN)\n",
    "    model = AutoModelForCausalLM.from_pretrained(\n",
    "        MODEL_NAME,\n",
    "        token=HF_TOKEN,\n",
    "        torch_dtype=torch.float16,\n",
    "        device_map=\"auto\",\n",
    "        low_cpu_mem_usage=True\n",
    "    )\n",
    "    print(\"Model loaded successfully!\\n\")\n",
    "    USE_LOCAL_MODEL = True\n",
    "except Exception as e:\n",
    "    print(f\"Could not load model locally: {e}\")\n",
    "    print(\"Falling back to Inference API until access is approved\\n\")\n",
    "    USE_LOCAL_MODEL = False\n",
    "    client = InferenceClient(api_key=HF_TOKEN)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "31837e44",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Campaign ready\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# CAMPAIGN SETUP\n",
    "\n",
    "org_name = input(\"Organization [Children's Education Fund]: \").strip() or \"Children's Education Fund\"\n",
    "cause = input(\"Cause [education for children]: \").strip() or \"providing education to underprivileged children\"\n",
    "amounts = input(\"Amounts [200,500,1000]: \").strip() or \"200, 500, 1000\"\n",
    "impact = input(\"Impact [₹200 = 5 kids supplies]: \").strip() or \"₹200 provides school supplies for 5 children for a month\"\n",
    "\n",
    "DONATION_CONTEXT = {\n",
    "    'organization': org_name,\n",
    "    'cause': cause,\n",
    "    'amounts': amounts,\n",
    "    'impact': impact\n",
    "}\n",
    "\n",
    "print(\"\\nCampaign ready\\n\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
