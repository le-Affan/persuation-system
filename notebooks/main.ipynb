{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "20e27425",
   "metadata": {},
   "source": [
    "ADAPTIVE PERSUASION SYSTEM - PRODUCTION VERSION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7b07bb25",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Affan\\Desktop\\Futwork\\Persuation_System\\.venv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All required libraries are installed and imported successfully.\n"
     ]
    }
   ],
   "source": [
    "!pip install -q transformers>=4.40.0 huggingface_hub>=0.23.0 torch\n",
    "!pip install -q textblob nltk pandas numpy matplotlib\n",
    "\n",
    "import torch, transformers, textblob, nltk, pandas, numpy, matplotlib\n",
    "print(\"All required libraries are installed and imported successfully.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "63d84b7e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Imports\n",
    "\n",
    "import os\n",
    "import json\n",
    "import re\n",
    "from datetime import datetime\n",
    "from typing import Dict, List, Tuple, Optional\n",
    "import random\n",
    "\n",
    "from huggingface_hub import InferenceClient, login\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "import torch\n",
    "from textblob import TextBlob\n",
    "import nltk\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import display, HTML\n",
    "\n",
    "\n",
    "# NLTK Downloads\n",
    "nltk.download('punkt', quiet=True)\n",
    "nltk.download('vader_lexicon', quiet=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "69af6cc9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Note: Environment variable`HF_TOKEN` is set and is the current active token independently from the token you've just configured.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Authenticated!\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Authentication\n",
    "\n",
    "HF_TOKEN = os.getenv(\"HF_TOKEN\")\n",
    "os.environ['HF_TOKEN'] = HF_TOKEN\n",
    "\n",
    "try:\n",
    "    login(token=HF_TOKEN, add_to_git_credential=False)\n",
    "    print(\"✅ Authenticated!\\n\")\n",
    "except Exception as e:\n",
    "    print(f\"❌ Failed: {e}\")\n",
    "    raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "95269a92",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "⚠️  Could not load model locally: You are trying to access a gated repo.\n",
      "Make sure to have access to it at https://huggingface.co/meta-llama/Llama-3.1-8B-Instruct.\n",
      "403 Client Error. (Request ID: Root=1-69626447-19f125892bef0f01563e7a17;db3e5856-8943-4894-86e0-2e6ea4b8e7b4)\n",
      "\n",
      "Cannot access gated repo for url https://huggingface.co/meta-llama/Llama-3.1-8B-Instruct/resolve/main/config.json.\n",
      "Your request to access model meta-llama/Llama-3.1-8B-Instruct is awaiting a review from the repo authors.\n",
      "Falling back to Inference API until access is approved\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Loading the llama 3.1 model\n",
    "\n",
    "MODEL_NAME = \"meta-llama/Llama-3.1-8B-Instruct\"\n",
    "\n",
    "try:\n",
    "    tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME, token=HF_TOKEN)\n",
    "    model = AutoModelForCausalLM.from_pretrained(\n",
    "        MODEL_NAME,\n",
    "        token=HF_TOKEN,\n",
    "        torch_dtype=torch.float16,\n",
    "        device_map=\"auto\",\n",
    "        low_cpu_mem_usage=True\n",
    "    )\n",
    "    print(\"✅ Model loaded successfully!\\n\")\n",
    "    USE_LOCAL_MODEL = True\n",
    "except Exception as e:\n",
    "    print(f\"⚠️  Could not load model locally: {e}\")\n",
    "    print(\"Falling back to Inference API until access is approved\\n\")\n",
    "    USE_LOCAL_MODEL = False\n",
    "    client = InferenceClient(api_key=HF_TOKEN)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31837e44",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
